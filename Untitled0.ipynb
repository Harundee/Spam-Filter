{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMkQsJnbqOda+WJojsoSe2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNcL5T7yhOjP","executionInfo":{"status":"ok","timestamp":1718902040959,"user_tz":-120,"elapsed":1677,"user":{"displayName":"Marcel El Din","userId":"01928633628146857320"}},"outputId":"01d4c535-f7d4-4c80-d621-d248ef67045e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import re\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","nltk.download('wordnet')\n","nltk.download('punkt')"]},{"cell_type":"code","source":["class DataLoader:\n","    def __init__(self, file_name):\n","        self.file_name = file_name\n","        self.df = None\n","        self.lemmatizer = WordNetLemmatizer()\n","\n","    def load_data(self):\n","        self.df = pd.read_csv(self.file_name)\n","        return self.df\n","\n","    def preprocess_text(self, text):\n","        text = re.sub(r'\\d+', '', text)  # usuwa liczby\n","        text = re.sub(r'\\W+', ' ', text)  # usuwa interpunkcjÄ™\n","        text = text.lower()  # usuwa wielkie litery\n","        text = ' '.join([self.lemmatizer.lemmatize(word) for word in text.split()])\n","        return text\n","\n","    def preprocess_data(self):\n","        self.df['text'] = self.df['text'].apply(self.preprocess_text)\n","        self.df.loc[self.df['label'] == 'spam', 'label'] = 0\n","        self.df.loc[self.df['label'] == 'ham', 'label'] = 1\n","        return self.df\n","\n","    def split_data(self, test_size=0.2, random_state=42):\n","        X = self.df['text']\n","        y = self.df['label'].astype('int')\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n","        return X_train, X_test, y_train, y_test\n","\n","\n","\n","data_loader = DataLoader('spam_ham_dataset.csv')\n","\n","data_loader.load_data()\n","data_loader.preprocess_data()\n","\n","X_train, X_test, y_train, y_test = data_loader.split_data()\n","\n","pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n","    ('clf', LogisticRegression(max_iter=1000))\n","])\n","\n","params = {\n","    'tfidf__max_features': [500, 1000, 2000, None],\n","    'clf__C': [0.1, 1.0, 10.0, 100],\n","    'clf__solver': ['liblinear', 'saga']\n","}\n","\n","gs = GridSearchCV(estimator=pipeline, param_grid=params, cv=5, scoring='accuracy', verbose=1)\n","gs.fit(X_train, y_train)\n","\n","\n","model = gs.best_estimator_\n","y_pred = model.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy}')\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')\n","print(f'F1-Score: {f1}')\n","print(f'Confusion Matrix:\\n{conf_matrix}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-tG23QFkhW2s","executionInfo":{"status":"ok","timestamp":1718903537614,"user_tz":-120,"elapsed":322006,"user":{"displayName":"Marcel El Din","userId":"01928633628146857320"}},"outputId":"19bd771d-7734-4e50-cf37-72c8cd74a0f9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 32 candidates, totalling 160 fits\n","Accuracy: 0.9903381642512077\n","Precision: 0.9945429740791268\n","Recall: 0.9918367346938776\n","F1-Score: 0.9931880108991826\n","Confusion Matrix:\n","[[296   4]\n"," [  6 729]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NRLYEQtvhiRH"},"execution_count":null,"outputs":[]}]}